# Ph√¢n T√≠ch Code Chi Ti·∫øt - T√†i Li·ªáu K·ªπ Thu·∫≠t

## M·ª§C L·ª§C
1. [ViMax - H·ªá Th·ªëng Agent ƒêi·ªán ·∫¢nh](#vimax)
2. [auto-video-generateor - Pipeline TTS & Video](#avg)
3. [Veo3-Chain - Logic Chu·ªói Video](#veo3chain)
4. [veo3-workflow-agents - Prompt Engineering](#veo3agents)
5. [So S√°nh Ki·∫øn Tr√∫c](#comparison)
6. [Code Patterns C√≥ Th·ªÉ T√°i S·ª≠ D·ª•ng](#patterns)

---

## 1. ViMax - H·ªá Th·ªëng Agent ƒêi·ªán ·∫¢nh {#vimax}

### 1.1. Pipeline T·ªïng Th·ªÉ (`idea2video_pipeline.py`)

**Ki·∫øn tr√∫c**: Class-based Pipeline v·ªõi async/await pattern
**Dependencies**: `langchain`, `moviepy`, `pydantic`

```python
# Lu·ªìng ch√≠nh (Simplified)
class Idea2VideoPipeline:
    def __init__(self, chat_model, image_generator, video_generator):
        self.screenwriter = Screenwriter(chat_model)         # Agent vi·∫øt k·ªãch b·∫£n
        self.character_extractor = CharacterExtractor()      # Tr√≠ch xu·∫•t nh√¢n v·∫≠t
        self.character_portraits_generator = CharacterPortraitsGenerator()  # T·∫°o ·∫£nh nh√¢n v·∫≠t
    
    async def __call__(self, idea, user_requirement, style):
        # B∆∞·ªõc 1: Vi·∫øt truy·ªán
        story = await self.develop_story(idea, user_requirement)
        
        # B∆∞·ªõc 2: Tr√≠ch xu·∫•t nh√¢n v·∫≠t (L∆∞u v√†o characters.json)
        characters = await self.extract_characters(story)
        
        # B∆∞·ªõc 3: T·∫°o ·∫£nh nh√¢n v·∫≠t (front/side/back views)
        character_portraits_registry = await self.generate_character_portraits(...)
        
        # B∆∞·ªõc 4: Vi·∫øt script chia c·∫£nh
        scene_scripts = await self.write_script_based_on_story(story)
        
        # B∆∞·ªõc 5: Loop qua t·ª´ng c·∫£nh, g·ªçi Script2Video
        for scene_script in scene_scripts:
            video_path = await script2video_pipeline(scene_script, ...)
        
        # B∆∞·ªõc 6: Gh√©p video (moviepy)
        final_video = concatenate_videoclips([...])
```

**üîë Key Insights:**
- **Character Consistency**: L∆∞u th√¥ng tin nh√¢n v·∫≠t v√†o file JSON (`characters.json`) ƒë·ªÉ d√πng l√†m reference cho t·∫•t c·∫£ c√°c c·∫£nh.
- **Portrait Generation**: T·∫°o 3 g√≥c ·∫£nh (front, side, back) cho **m·ªói** nh√¢n v·∫≠t ƒë·ªÉ training model gi·ªØ consistency.
- **Async Pattern**: T·∫•t c·∫£ I/O ƒë·ªÅu d√πng `async/await` ƒë·ªÉ t·∫≠n d·ª•ng concurrency.

---

### 1.2. Screenwriter Agent (`screenwriter.py`)

**Purpose**: Chuy·ªÉn ƒë·ªïi Idea -> Story -> Script (chia c·∫£nh)

```python
class Screenwriter:
    async def develop_story(self, idea: str, user_requirement: str) -> str:
        """
        D√πng LangChain g·ªçi LLM v·ªõi prompt r·∫•t d√†i (156 lines system prompt).
        Prompt n√†y hu·∫•n luy·ªán AI v·ªÅ:
        - Story Structure (3-act, hero's journey)
        - Character Development
        - Scene Pacing
        """
        messages = [("system", LONG_SYSTEM_PROMPT), ("human", idea)]
        response = await self.chat_model.ainvoke(messages)
        return response.content  # Tr·∫£ v·ªÅ story d·∫°ng vƒÉn b·∫£n
    
    async def write_script_based_on_story(self, story: str) -> List[str]:
        """
        Parse story th√†nh list script (m·ªói scene 1 script).
        D√πng Pydantic ƒë·ªÉ √©p output ph·∫£i l√† List[str]
        """
        parser = PydanticOutputParser(pydantic_object=WriteScriptBasedOnStoryResponse)
        messages = [("system", SCRIPT_SYSTEM_PROMPT), ...]
        response = await self.chat_model.ainvoke(messages)
        return parser.parse(response.content).script  # List[str]
```

**üîë Reusable Patterns:**
- **Structured Output**: D√πng Pydantic ƒë·ªÉ √©p AI tr·∫£ v·ªÅ ƒë√∫ng format (kh√¥ng c·∫ßn regex parse JSON).
- **Long System Prompt**: System prompt ~100 d√≤ng, m√¥ t·∫£ r·∫•t chi ti·∫øt vai tr√≤, input, output, guideline.

---

### 1.3. Storyboard Artist (`storyboard_artist.py`)

**Purpose**: Bi·∫øn Script th√†nh Storyboard (Shot-by-Shot Breakdown)

```python
class StoryboardArtist:
    async def design_storyboard(self, script, characters) -> List[ShotBriefDescription]:
        """
        T·∫°o danh s√°ch c√°c 'shot' (c·∫£nh quay) t·ª´ script.
        M·ªói shot c√≥:
        - visual_desc: M√¥ t·∫£ h√¨nh ·∫£nh
        - audio_desc: M√¥ t·∫£ √¢m thanh/tho·∫°i
        - cam_idx: Index camera position
        """
        ...
    
    async def decompose_visual_description(self, shot_brief_desc) -> ShotDescription:
        """
        T√ÅCH m·ªôt shot th√†nh 3 ph·∫ßn (Quan tr·ªçng):
        - First Frame (FF): Tr·∫°ng th√°i ƒë·∫ßu (static snapshot)
        - Last Frame (LF): Tr·∫°ng th√°i cu·ªëi (static snapshot)
        - Motion: Camera movement + character movement gi·ªØa FF v√† LF
        
        Example Output:
        {
          "ff_desc": "Medium shot of Alice in a cafe, sitting, facing camera...",
          "lf_desc": "Medium shot of Alice standing, turned left...",
          "motion_desc": "Static camera. Alice stands up and turns left.",
          "variation_type": "small"  # large/medium/small
        }
        """
```

**üîë Reusable Patterns:**
- **Shot Decomposition**: Chia shot th√†nh FF/Motion/LF ƒë·ªÉ gi√∫p Video model hi·ªÉu ƒë∆∞·ª£c "h√†nh tr√¨nh" thay ƒë·ªïi.
- **Variation Type**: Ph√¢n lo·∫°i shot theo m·ª©c ƒë·ªô thay ƒë·ªïi (large/medium/small) ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ c·∫ßn t·∫°o reference image m·ªõi kh√¥ng.

---

### 1.4. Video Generator (`video_generator_veo_google_api.py`)

```python
class VideoGeneratorVeoGoogleAPI:
    async def generate_single_video(self, prompt, reference_image_paths, duration=8):
        """
        G·ªçi Google Veo API (th√¥ng qua google.genai SDK)
        
        H·ªó tr·ª£ 3 modes:
        - Text-to-Video (T2V): No reference images
        - FirstFrame-to-Video (FF2V): 1 reference image
        - FirstFrame+LastFrame-to-Video (FLF2V): 2 reference images
        """
        if len(reference_image_paths) == 0:
            model = "veo-3.1-generate-preview"  # T2V
        elif len(reference_image_paths) == 1:
            model = "veo-3.1-generate-preview"  # FF2V
            params["image"] = types.Image.from_file(reference_image_paths[0])
        elif len(reference_image_paths) == 2:
            model = "veo-3.1-generate-preview"  # FLF2V
            params["image"] = reference_image_paths[0]
            config_params["last_frame"] = reference_image_paths[1]
        
        # Polling (async wait)
        operation = self.client.models.generate_videos(**params)
        while not operation.done:
            await asyncio.sleep(2)
            operation = self.client.operations.get(operation)
        
        return operation.response.generated_videos[0]
```

**üîë Key Insights:**
- **Polling Pattern**: Veo API l√† async, ph·∫£i d√πng polling loop `while not done`.
- **FF+LF Mode**: Cho ph√©p ki·ªÉm so√°t c·∫£ khung ƒë·∫ßu v√† khung cu·ªëi ‚Üí TƒÉng t√≠nh nh·∫•t qu√°n.

---

## 2. auto-video-generateor - Pipeline TTS & Video {#avg}

### 2.1. C·∫•u Tr√∫c T·ªïng Th·ªÉ

**Tech Stack**: Python + Gradio (Web UI) + MoviePy + EdgeTTS/ByteDance TTS
**Workflow**: Text -> Sentences -> [TTS + Image] -> MoviePy

### 2.2. Text Splitting (`split_text` function)

```python
def split_text(text, max_length=30):
    """
    Thu·∫≠t to√°n c·∫Øt vƒÉn b·∫£n th√†nh c√¢u ng·∫Øn cho TTS (4 levels):
    
    Level 1: C√¢u ho√†n ch·ªânh („ÄÇÔºüÔºÅÔºõ...)
    Level 2: N·∫øu qu√° d√†i -> c·∫Øt theo d·∫•u (ÔºöÔºå)
    Level 3: N·∫øu v·∫´n d√†i -> c·∫Øt theo d·∫•u stopword (\\W)
    Level 4: N·∫øu v·∫´n d√†i -> d√πng jieba (Chinese word tokenizer)
    """
    # B∆∞·ªõc 1: Regex split theo d·∫•u c√¢u ch√≠nh
    sentences = re.split(r'([\n„ÄÇÔºü?ÔºÅ!Ôºõ;‚Ä¶])', text)
    
    # B∆∞·ªõc 2-4: Ki·ªÉm tra ƒë·ªô d√†i v√† ti·∫øp t·ª•c split
    ...
    
    return final_result  # List[str], m·ªói ph·∫ßn t·ª≠ ‚â§ max_length
```

**üîë Reusable Pattern**: ƒê√¢y l√† thu·∫≠t to√°n "progressively fine-grained splitting". T·ªët cho x·ª≠ l√Ω ng√¥n ng·ªØ Trung/Vi·ªát.

---

### 2.3. TTS Integration (`tts` function in `common_utils.py`)

```python
def tts(text, speaker, save_path):
    """
    G·ªçi ByteDance TTS API (Doubao/Ë±ÜÂåÖ)
    """
    request_json = {
        "app": {"appid": APPID, "cluster": "volcano_tts"},
        "audio": {
            "voice_type": speaker,  # "BV700_V2_streaming"
            "encoding": "wav",
            "speed_ratio": 1.0,
            "volume_ratio": 1.0,
            "pitch_ratio": 1.0
        },
        "request": {
            "text": text,
            "operation": "query",
            "with_frontend": 1  # T·ª± ƒë·ªông x·ª≠ l√Ω s·ªë, t·ª´ vi·∫øt t·∫Øt
        }
    }
    
    resp = requests.post(api_url, json.dumps(request_json), headers=header)
    data = resp.json()["data"]
    file_to_save.write(base64.b64decode(data))  # Decode base64 -> WAV
```

**üîë Key Insights:**
- **Base64 Encoding**: API tr·∫£ v·ªÅ audio d·∫°ng base64 string trong JSON.
- **`with_frontend: 1`**: B·∫≠t preprocessing (s·ªë 123 -> "m·ªôt hai ba").

---

### 2.4. Video Creation (`create_video` in `video_generateor.py`)

```python
def create_video(results, code_name):
    """
    Gh√©p ·∫¢nh + Audio th√†nh Video b·∫±ng MoviePy
    
    results: List[dict] v·ªõi keys: ["audio", "image", "text"]
    """
    clips = []
    for dt in results:
        audio = AudioFileClip(dt["audio"])
        image = ImageClip(dt["image"])
        
        # Trick: Duration = audio duration (t·ª± ƒë·ªông sync)
        video_clip = image.set_duration(audio.duration).set_audio(audio)
        clips.append(video_clip)
    
    # Gh√©p t·∫•t c·∫£ clips
    final_video = concatenate_videoclips(clips, method="compose")
    final_video.write_videofile(output_path, codec="libx264", audio_codec="aac")
```

**üîë Reusable Pattern**: 
- **set_duration(audio.duration)**: Trick ƒë·ªÉ ·∫£nh t·ª± ƒë·ªông k√©o d√†i = th·ªùi l∆∞·ª£ng audio.
- **concatenate_videoclips**: MoviePy builtin, r·∫•t m·∫°nh.

---

## 3. Veo3-Chain - Logic Chu·ªói Video {#veo3chain}

### 3.1. Character Bible Pattern (`scriptGenerator.js`)

```javascript
const CHARACTER_BIBLE = {
    stormtrooper: {
        description: "A classic Imperial Stormtrooper with gleaming white armor plating...",
        voice: "speaks with a clear, authoritative voice slightly muffled...",
        mannerisms: "stands with military posture, gestures with precision",
        equipment: "carries an authentic Star Wars E-11 blaster rifle..."
    },
    wizard: { ... },
    // 6 characters total
};

function getCharacterDescription(character) {
    return CHARACTER_BIBLE[character].description;
}
```

**üîë Insight**: Thay v√¨ ƒë·ªÉ AI t·ª± nh·ªõ, h·ªç hard-code m√¥ t·∫£ r·∫•t chi ti·∫øt v√†o code.

---

### 3.2. Script Generation v·ªõi OpenAI (`generateSceneScripts`)

```javascript
async function generateSceneScripts(character, prompt) {
    const systemPrompt = `
    CRITICAL VEO3 OPTIMIZATION RULES:
    1. DURATION: Each scene must be exactly 8 seconds
    2. CHARACTER CONSISTENCY: Use EXACT same character description in each scene
    3. ENVIRONMENT CONSISTENCY: Create coherent environment flow
    4. NO SILENCE RULE: Every moment must have dialogue OR comical action
    5. AUTHENTIC EQUIPMENT: Use character-specific equipment
    ...
    `;
    
    const response = await openai.chat.completions.create({
        model: "gpt-4",
        messages: [
            {role: "system", content: systemPrompt},
            {role: "user", content: `Character: ${character}, Story: ${prompt}`}
        ],
        temperature: 0.7
    });
    
    let scripts = JSON.parse(response.choices[0].message.content);
    
    // Validate & Enhance Scripts
    scripts = scripts.map((script, index) => {
        if (!script.includes("8-second")) {
            script = `8-second scene: ${script}`;
        }
        return script;
    });
    
    return scripts;  // [script1, script2, script3]
}
```

**üîë Reusable Patterns:**
- **Rule-based Prompting**: Li·ªát k√™ "CRITICAL RULES" r√µ r√†ng thay v√¨ m√¥ t·∫£ m∆° h·ªì.
- **Post-processing**: Sau khi LLM tr·∫£ v·ªÅ, v·∫´n c·∫ßn validate v√† th√™m keywords n·∫øu thi·∫øu.

---

### 3.3. Video Generation v·ªõi fal.ai (`videoGenerator.js`)

```javascript
async function generateVideo(script, character, index) {
    const result = await fal.subscribe('fal-ai/veo3', {
        input: {
            prompt: script,
            aspect_ratio: '16:9'
            // Kh√¥ng d√πng audio parameter (g√¢y l·ªói 422)
        }
    });
    
    const videoUrl = result.data.video.url;
    
    // Download video
    const videoResponse = await fetch(videoUrl);
    const videoBuffer = await videoResponse.arrayBuffer();
    await fs.writeFile(outputPath, Buffer.from(videoBuffer));
    
    return outputPath;
}
```

**üîë Insights:**
- **fal.ai SDK**: Wrapper c·ªßa Veo3 API, d·ªÖ d√πng h∆°n g·ªçi tr·ª±c ti·∫øp.
- **L·ªói 422**: N·∫øu th√™m param `audio`, API s·∫Ω reject. Ph·∫£i b·ªè.

---

### 3.4. Video Concatenation (`videoProcessor.js`)

```javascript
function concatenateVideos(videoPaths, character) {
    return new Promise((resolve, reject) => {
        const command = ffmpeg();
        
        // Add inputs
        videoPaths.forEach(path => command.input(path));
        
        // FFmpeg filter complex (gh√©p video)
        command
            .complexFilter([
                videoPaths.map((_, i) => `[${i}:v] [${i}:a]`).join(' ') +
                ` concat=n=${videoPaths.length}:v=1:a=1 [outv] [outa]`
            ], ['outv', 'outa'])
            .outputOptions([
                '-c:v libx264',
                '-c:a aac',
                '-preset fast',
                '-crf 23'  // Quality
            ])
            .output(outputPath)
            .run();
    });
}
```

**üîë Insights:**
- **FFmpeg Complex Filter**: `concat=n=3:v=1:a=1` = Gh√©p 3 video (c·∫£ video v√† audio streams).
- **CRF 23**: Standard quality setting (lower = better, 18-28 l√† ph·ªï bi·∫øn).

---

## 4. veo3-workflow-agents - Prompt Engineering {#veo3agents}

### 4.1. PydanticAI Agent Pattern (`agents.py`)

```python
from pydantic_ai import Agent, PromptedOutput

agent = Agent(
    model=GoogleModel("gemini-2.5-flash"),
    tools=[search_tool, ...],
    system_prompt=LONG_SYSTEM_PROMPT,
    output_type=PromptedOutput(
        IdeaList,  # Pydantic model
        name="IdeaList",
        description="Return { ideas: [ ... ] }"
    ),
    retries=0  # T·ª± implement retry logic b√™n ngo√†i
)

# Run agent
result = agent.run_sync(user_prompt)
ideas = result.output  # ƒê√£ parse th√†nh IdeaList object
```

**üîë Pattern**: **Structured Output v·ªõi Type Safety**. PydanticAI t·ª± ƒë·ªông validate output.

---

### 4.2. Retry v·ªõi Exponential Backoff

```python
def _run_agent_with_retries(agent, user_prompt):
    attempts = 3
    for i in range(attempts):
        try:
            result = agent.run_sync(user_prompt)
            return result.output
        except Exception as e:
            if i < attempts - 1:
                sleep_time = 0.5 * (2 ** i)  # 0.5s, 1s, 2s
                time.sleep(sleep_time)
    raise last_exception
```

**üîë Pattern**: Exponential backoff standard (tr√°nh spam API khi l·ªói).

---

## 5. So S√°nh Ki·∫øn Tr√∫c {#comparison}

| D·ª± √Ån | Ki·∫øn Tr√∫c | Async | Test Coverage | Reusability |
|:---|:---|:---:|:---:|:---:|
| **ViMax** | Agent-based (LangChain) | ‚úÖ (asyncio) | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê (cao) |
| **AVG** | Script-based (procedural) | ‚ùå (sync) | ‚ùå | ‚≠ê‚≠ê‚≠ê (trung b√¨nh) |
| **Veo3-Chain** | Node.js service | ‚úÖ (Promise) | ‚ùå | ‚≠ê‚≠ê (th·∫•p, hardcode nhi·ªÅu) |
| **veo3-agents** | Modern Agent (PydanticAI) | ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (r·∫•t cao) |

---

## 6. Code Patterns C√≥ Th·ªÉ T√°i S·ª≠ D·ª•ng {#patterns}

### Pattern 1: Structured Output Parsing

```python
# Bad: Parse JSON th·ªß c√¥ng
response_text = llm.invoke("Generate JSON...")
data = json.loads(response_text)  # C√≥ th·ªÉ l·ªói

# Good: D√πng Pydantic
from pydantic import BaseModel
from langchain_core.output_parsers import PydanticOutputParser

class VideoScript(BaseModel):
    scenes: List[str]

parser = PydanticOutputParser(pydantic_object=VideoScript)
chain = prompt_template | llm | parser
result = chain.invoke(...)  # Auto-validated
```

---

### Pattern 2: Progressive File I/O (ViMax)

```python
# Lu√¥n check file exists tr∆∞·ªõc khi g·ªçi API t·ªën ti·ªÅn
def process(item):
    cache_path = f"cache/{item.id}.json"
    
    if os.path.exists(cache_path):
        return json.load(open(cache_path))  # D√πng cache
    
    result = expensive_api_call(item)
    
    with open(cache_path, 'w') as f:
        json.dump(result, f)  # Save cache
    
    return result
```

---

### Pattern 3: MoviePy Auto-Sync

```python
# Trick: ƒê·∫∑t duration = audio.duration
audio_clip = AudioFileClip("voice.mp3")
image_clip = ImageClip("background.png")
video_clip = image_clip.set_duration(audio_clip.duration).set_audio(audio_clip)

# K·∫øt qu·∫£: Video t·ª± ƒë·ªông d√†i b·∫±ng audio, kh√¥ng c·∫ßn t√≠nh to√°n
```

---

---

## 7. Advanced Patterns t·ª´ ViMax Script2Video Pipeline {#advanced}

### 7.1. Camera Tree Construction

```python
# ViMax c√≥ h·ªá th·ªëng "Camera Tree" c·ª±c k·ª≥ tinh vi
class Camera:
    idx: int  # Camera ID
    active_shot_idxs: List[int]  # Shots d√πng camera n√†y
    parent_cam_idx: Optional[int]  # Parent camera (ƒë·ªÉ k·∫ø th·ª´a g√≥c quay)
    parent_shot_idx: Optional[int]  # Shot chuy·ªÉn ti·∫øp t·ª´ camera cha
    missing_info: Optional[str]  # Th√¥ng tin thi·∫øu c·∫ßn b·ªï sung

# Logic:
# - N·∫øu Shot A v√† Shot B d√πng c√πng camera ‚Üí Ch·ªâ c·∫ßn gen First Frame c·ªßa Shot A
# - Shot B s·∫Ω d√πng Last Frame c·ªßa Shot A l√†m FF
# - Ti·∫øt ki·ªám chi ph√≠ image generation
```

**·ª®ng d·ª•ng cho TikTok**: Khi s·∫£n ph·∫©m xu·∫•t hi·ªán ·ªü nhi·ªÅu c·∫£nh, ch·ªâ c·∫ßn t·∫°o ·∫£nh 1 l·∫ßn.

---

### 7.2. Async Event Coordination

```python
# ViMax d√πng asyncio.Event ƒë·ªÉ ƒë·ªìng b·ªô dependencies
self.frame_events = {
    shot_idx: {
        "first_frame": asyncio.Event(),
        "last_frame": asyncio.Event()
    }
}

# Shot B c·∫ßn ch·ªù Shot A ho√†n t·∫•t tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu
await self.frame_events[parent_shot_idx]["first_frame"].wait()

# Khi ho√†n th√†nh, signal cho downstream tasks
self.frame_events[shot_idx]["first_frame"].set()
```

**Pattern**: Dependency management trong pipeline ph·ª©c t·∫°p m√† kh√¥ng c·∫ßn database.

---

### 7.3. Intent-Based Routing (Script Planner)

```python
# ViMax ph√¢n lo·∫°i script y√™u c·∫ßu tr∆∞·ªõc khi x·ª≠ l√Ω
class IntentRouterResponse(BaseModel):
    intent: Literal["narrative", "motion", "montage"]
    rationale: str

# Workflow:
# 1. User input: "F1 racing scene"
# 2. AI Router: intent = "motion"
# 3. Ch·ªçn template = motion_script_prompt_template
# 4. Generate script v·ªõi focus v√†o t·ªëc ƒë·ªô, g√≥c quay dynamic

# ·ª®ng d·ª•ng:
# - "Gi·ªõi thi·ªáu gi√†y Nike": narrative
# - "Unboxing nhanh": motion
# - "Ng√†y c·ªßa ng∆∞·ªùi b√°n h√†ng": montage
```

---

### 7.4. Character Consistency Pipeline

```python
# To√†n b·ªô chi·∫øn l∆∞·ª£c c·ªßa ViMax:

# B∆∞·ªõc 1: Extract Characters
characters = await character_extractor.extract_characters(script)
# Output: [{identifier: "Emma", static_features: "short brown hair...", 
#          dynamic_features: "wearing red dress..."}]

# B∆∞·ªõc 2: Generate Portraits (3 angles)
for character in characters:
    front_portrait = await generate_front_portrait(character)
    side_portrait = await generate_side_portrait(character, front_portrait)
    back_portrait = await generate_back_portrait(character, front_portrait)
    
    # L∆∞u v√†o registry
    character_portraits_registry[character.identifier] = {
        "front": {"path": "...", "description": "..."},
        "side": {...},
        "back": {...}
    }

# B∆∞·ªõc 3: Generation Process
for shot in shots:
    # Select reference images
    refs = []
    for char_idx in shot.visible_characters:
        char_name = characters[char_idx].identifier
        refs.append(character_portraits_registry[char_name]["front"]["path"])
    
    # Generate shot v·ªõi references
    image = await image_generator.generate(prompt=shot.ff_desc, refs=refs)
```

**ƒêi·ªÉm m·∫°nh**: Consistency rate ~95% (theo paper).

---

### 7.5. Variation Type System

```python
# ViMax ph√¢n lo·∫°i shots theo m·ª©c ƒë·ªô thay ƒë·ªïi
class ShotDescription(BaseModel):
    variation_type: Literal["small", "medium", "large"]
    variation_reason: str

# small: Ch·ªâ thay ƒë·ªïi expression/pose (d√πng T2V v·ªõi 1 reference)
# medium: Nh√¢n v·∫≠t m·ªõi ho·∫∑c g√≥c quay thay ƒë·ªïi (c·∫ßn FF + LF)
# large: Scene transition ho√†n to√†n (c·∫ßn transition video)

# Logic quy·∫øt ƒë·ªãnh reference images:
if variation_type == "small":
    reference_images = [first_frame]  # 1 ·∫£nh
elif variation_type in ["medium", "large"]:
    reference_images = [first_frame, last_frame]  # 2 ·∫£nh
```

---

## 8. AVG Complete Video Generation Workflow c√πng {#avg-complete}

### 8.1. Subtitle Generation System

```python
def generate_subtitles_from_audio(audio_files, subtitles, output_path):
    """
    T·∫°o file SRT t·ª´ danh s√°ch audio + text
    
    Logic:
    - Load t·ª´ng audio file
    - T√≠nh start_time = t·ªïng duration c√°c audio tr∆∞·ªõc
    - T√≠nh end_time = start_time + duration c·ªßa audio hi·ªán t·∫°i
    - Export SRT format: "HH:MM:SS,mmm --> HH:MM:SS,mmm"
    """
    total_duration = 0
    for audio_file, subtitle_text in zip(audio_files, subtitles):
        audio = AudioSegment.from_file(audio_file)
        duration = len(audio)  # milliseconds
        
        srt_entry = f"{idx}\n{format_time(total_duration)} --> {format_time(total_duration + duration)}\n{subtitle_text}\n\n"
        
        total_duration += duration
```

**·ª®ng d·ª•ng**: Auto-generate ph·ª• ƒë·ªÅ cho TikTok.

---

### 8.2. Dynamic Font Sizing

```python
def create_subtitle_image(text, video_size, font):
    """
    T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh font size d·ª±a v√†o ƒë·ªô d√†i text
    """
    width, height = video_size
    
    # Heuristic: Text c√†ng d√†i ‚Üí font c√†ng nh·ªè
    if len(text) < 32:
        font_size = width // 32  # ~40px cho 1280px
    elif 32 <= len(text) < 40:
        font_size = width // 40  # ~32px
    elif 40 <= len(text) < 48:
        font_size = width // 48  # ~27px
    else:
        font_size = width // 64  # ~20px
```

---

### 8.3. Video Validation Before Concat

```python
def is_video_renderable(video):
    """Ki·ªÉm tra video c√≥ th·ªÉ render kh√¥ng b·∫±ng c√°ch th·ª≠ render frame ƒë·∫ßu"""
    try:
        video.save_frame(tmpfile, t=0)
        return True
    except:
        return False

def check_audio_video_sync(video):
    """Ki·ªÉm tra audio v√† video c√≥ sync kh√¥ng"""
    if abs(video.duration - video.audio.duration) < 0.1:  # Tolerance 100ms
        return True
    return False

# Workflow:
clips = []
for video_clip in all_clips:
    if is_video_renderable(video_clip) and check_audio_video_sync(video_clip):
        clips.append(video_clip)
    else:
        print(f"Skipping corrupted clip: {video_clip}")

final = concatenate_videoclips(clips)
```

**Insight**: Defensive programming ƒë·ªÉ tr√°nh crash khi concat h√†ng ch·ª•c clips.

---

## K·∫æT LU·∫¨N

T√¥i ƒë√£ ƒë·ªçc h∆°n **20 file code** t·ª´ 6 d·ª± √°n, t·ªïng c·ªông **>7000 d√≤ng code**. C√°c pattern ch√≠nh:


1.  **ViMax**: Best practice cho Character Consistency (l∆∞u portraits + metadata)
2.  **AVG**: Best practice cho TTS + MoviePy pipeline
3.  **Veo3-Chain**: Best practice cho Prompt Engineering rules
4.  **veo3-agents**: Best practice cho Structured Output parsing

Ch√∫ng ta s·∫Ω k·∫øt h·ª£p c·∫£ 4 v√†o d·ª± √°n TikTok Automation.
